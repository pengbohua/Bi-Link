
@inproceedings{daza2021inductive,
  title={Inductive entity representations from text via link prediction},
  author={Daza, Daniel and Cochez, Michael and Groth, Paul},
  booktitle={Proceedings of the Web Conference 2021},
  pages={798--808},
  year={2021}
}
@inproceedings{teru2020inductive,
  title={Inductive relation prediction by subgraph reasoning},
  author={Teru, Komal and Denis, Etienne and Hamilton, Will},
  booktitle={International Conference on Machine Learning},
  pages={9448--9457},
  year={2020},
  organization={PMLR}
}
@inproceedings{dong2021rare,
  title={Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision},
  author={Dong, Hang and Su{\'a}rez-Paniagua, V{\'\i}ctor and Zhang, Huayu and Wang, Minhong and Whitfield, Emma and Wu, Honghan},
  booktitle={2021 43rd Annual International Conference of the IEEE Engineering in Medicine \& Biology Society (EMBC)},
  pages={2294--2298},
  year={2021},
  organization={IEEE}
}
 @inproceedings{han2018openke,
   title={OpenKE: An Open Toolkit for Knowledge Embedding},
   author={Han, Xu and Cao, Shulin and Lv Xin and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Li, Juanzi},
   booktitle={Proceedings of EMNLP},
   year={2018}
 }
@inproceedings{yang2022reinforcement,
  title={Reinforcement subgraph reasoning for fake news detection},
  author={Yang, Ruichao and Wang, Xiting and Jin, Yiqiao and Li, Chaozhuo and Lian, Jianxun and Xie, Xing},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={2253--2262},
  year={2022}
}
 @article{qu2019probabilistic,
  title={Probabilistic logic neural networks for reasoning},
  author={Qu, Meng and Tang, Jian},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{logeswaran2019zero,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}
@article{lehmann2015dbpedia,
  title={Dbpedia--a large-scale, multilingual knowledge base extracted from wikipedia},
  author={Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, S{\"o}ren and others},
  journal={Semantic web},
  volume={6},
  number={2},
  pages={167--195},
  year={2015},
  publisher={IOS Press}
}

@article{gesese2021survey,
  title={A survey on knowledge graph embeddings with literals: Which model links better literal-ly?},
  author={Gesese, Genet Asefa and Biswas, Russa and Alam, Mehwish and Sack, Harald},
  journal={Semantic Web},
  volume={12},
  number={4},
  pages={617--647},
  year={2021},
  publisher={IOS Press}
}
@article{wang2021kepler,
  title={KEPLER: A unified model for knowledge embedding and pre-trained language representation},
  author={Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={176--194},
  year={2021},
  publisher={MIT Press}
}
@inproceedings{toutanova2015representing,
  title={Representing text for joint embedding of text and knowledge bases},
  author={Toutanova, Kristina and Chen, Danqi and Pantel, Patrick and Poon, Hoifung and Choudhury, Pallavi and Gamon, Michael},
  booktitle={Proceedings of the 2015 conference on empirical methods in natural language processing},
  pages={1499--1509},
  year={2015}
}
@article{bordes2013translating,
  title={Translating embeddings for modeling multi-relational data},
  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@article{joshi2020spanbert,
  title={Spanbert: Improving pre-training by representing and predicting spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={64--77},
  year={2020},
  publisher={MIT Press}
}
@inproceedings{khattab2020colbert,
  title={Colbert: Efficient and effective passage search via contextualized late interaction over bert},
  author={Khattab, Omar and Zaharia, Matei},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={39--48},
  year={2020}
}
@inproceedings{Sun2018RotatEKG,
  title={RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space},
  author={Zhiqing Sun and Zhihong Deng and Jian-Yun Nie and Jian Tang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:67855617}
}

@inproceedings{wang2021structure,
  title={Structure-augmented text representation learning for efficient knowledge graph completion},
  author={Wang, Bo and Shen, Tao and Long, Guodong and Zhou, Tianyi and Wang, Ying and Chang, Yi},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1737--1748},
  year={2021}
}
@inproceedings{zhu2019graphvite,
  title={Graphvite: A high-performance cpu-gpu hybrid system for node embedding},
  author={Zhu, Zhaocheng and Xu, Shizhen and Tang, Jian and Qu, Meng},
  booktitle={The World Wide Web Conference},
  pages={2494--2504},
  year={2019}
}
@inproceedings{trouillon2016complex,
  title={Complex embeddings for simple link prediction},
  author={Trouillon, Th{\'e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, {\'E}ric and Bouchard, Guillaume},
  booktitle={International conference on machine learning},
  pages={2071--2080},
  year={2016},
  organization={PMLR}
}
@electronic{lucenescoring,
  added-at = {2022-09-22T12:00:00.000+0200},
  author = {Apache Software},
  description = {Apache Lucene - Scoring},
  keywords = {lucene ranking relevance scoring},
  timestamp = {2022-09-22T12:00:00.000+0200},
  title = {Apache Lucene - Scoring},
  url = {https://github.com/apache/lucene},
  year = 2022
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@inproceedings{gutmann2010noise,
  title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={297--304},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}
@inproceedings{wang-etal-2022-gpl,
    title = "{GPL}: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval",
    author = "Wang, Kexin  and
      Thakur, Nandan  and
      Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.168",
    doi = "10.18653/v1/2022.naacl-main.168",
    pages = "2345--2360",
    abstract = "Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method \textit{Generative Pseudo Labeling} (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at \url{https://github.com/UKPLab/gpl}.",
}
@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}
@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:52967399}
}
@inproceedings{malaviya2020commonsense,
  title={Commonsense knowledge base completion with structural and semantic context},
  author={Malaviya, Chaitanya and Bhagavatula, Chandra and Bosselut, Antoine and Choi, Yejin},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={03},
  pages={2925--2933},
  year={2020}
}
@book{dupriez1991dictionary,
  title={A dictionary of literary devices: Gradus, AZ},
  author={Dupriez, Bernard Marie},
  year={1991},
  publisher={University of Toronto Press}
}
@inproceedings{copestake2002multiword,
  title={Multiword expressions: linguistic precision and reusability.},
  author={Copestake, Ann A and Lambeau, Fabre and Villavicencio, Aline and Bond, Francis and Baldwin, Timothy and Sag, Ivan A and Flickinger, Dan},
  booktitle={LREC},
  year={2002}
}
@article{joshi2017automatic,
  title={Automatic sarcasm detection: A survey},
  author={Joshi, Aditya and Bhattacharyya, Pushpak and Carman, Mark J},
  journal={ACM Computing Surveys (CSUR)},
  volume={50},
  number={5},
  pages={1--22},
  year={2017},
  publisher={ACM New York, NY, USA}
}
@article{kuccuk2020stance,
  title={Stance detection: A survey},
  author={K{\"u}{\c{c}}{\"u}k, Dilek and Can, Fazli},
  journal={ACM Computing Surveys (CSUR)},
  volume={53},
  number={1},
  pages={1--37},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@inproceedings{kannangara2018mining,
  title={Mining twitter for fine-grained political opinion polarity classification, ideology detection and sarcasm detection},
  author={Kannangara, Sandeepa},
  booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
  pages={751--752},
  year={2018}
}
@article{misra2019sarcasm,
  title = {Sarcasm Detection using News Headlines Dataset},
  journal = {AI Open},
  volume = {4},
  pages = {13-18},
  year = {2023},
  issn = {2666-6510},
  doi = {https://doi.org/10.1016/j.aiopen.2023.01.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2666651023000013},
  author = {Rishabh Misra and Prahal Arora},
}
@article{potamias2020transformer,
  title={A transformer-based approach to irony and sarcasm detection},
  author={Potamias, Rolandos Alexandros and Siolas, Georgios and Stafylopatis, Andreas-Georgios},
  journal={Neural Computing and Applications},
  volume={32},
  pages={17309--17320},
  year={2020},
  publisher={Springer}
}
@article{vrandevcic2014wikidata,
  title={Wikidata: a free collaborative knowledgebase},
  author={Vrande{\v{c}}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  journal={Communications of the ACM},
  volume={57},
  number={10},
  pages={78--85},
  year={2014},
  publisher={ACM New York, NY, USA}
}
@article{stance-graph-sarcasm,
author = {Zhang, Yazhou and Ma, Dan and Tiwari, Prayag and Zhang, Chen and Masud, Mehedi and Shorfuzzaman, Mohammad and Song, Dawei},
title = {Stance-Level Sarcasm Detection with BERT and Stance-Centered Graph Attention Networks},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3533430},
doi = {10.1145/3533430},
abstract = {Computational Linguistics (CL) associated with the Internet of Multimedia Things (IoMT)-enabled multimedia computing applications brings several research challenges, such as real-time speech understanding, deep fake video detection, emotion recognition, home automation, and so on. Due to the emergence of machine translation, CL solutions have increased tremendously for different natural language processing (NLP) applications. Nowadays, NLP-enabled IoMT is essential for its success. Sarcasm detection, a recently emerging artificial intelligence (AI) and NLP task, aims at discovering sarcastic, ironic, and metaphoric information implied in texts that are generated in the IoMT. It has drawn much attention from the AI and IoMT research community. The advance of sarcasm detection and NLP techniques will provide a cost-effective, intelligent way to work together with machine devices and high-level human-to-device interactions. However, existing sarcasm detection approaches neglect the hidden stance behind texts, thus insufficient to exploit the full potential of the task. Indeed, the stance, i.e., whether the author of a text is in favor of, against, or neutral toward the proposition or target talked in the text, largely determines the text’s actual sarcasm orientation. To fill the gap, in this research, we propose a new task: stance-level sarcasm detection (SLSD), where the goal is to uncover the author’s latent stance and based on it to identify the sarcasm polarity expressed in the text. We then propose an integral framework, which consists of Bidirectional Encoder Representations from Transformers (BERT) and a novel stance-centered graph attention networks (SCGAT). Specifically, BERT is used to capture the sentence representation, and SCGAT is designed to capture the stance information on specific target. Extensive experiments are conducted on a Chinese sarcasm sentiment dataset we created and the SemEval-2018 Task 3 English sarcasm dataset. The experimental results prove the effectiveness of the SCGAT framework over state-of-the-art baselines by a large margin.},
journal = {ACM Trans. Internet Technol.},
month = {may},
articleno = {27},
numpages = {21},
keywords = {artificial intelligence, graph attention networks, stance extraction, Sarcasm detection}
}
@inproceedings{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
}
@misc{englishcorpora,
author={Davies, Mark},
  title = {English Corpora},
  url = {https://www.english-corpora.org},
  urldate = {2023-03-20},
  year={2015},
}
@article{yang2014embedding,
  title={Embedding Entities and Relations for Learning and Inference in Knowledge Bases},
  author={Bishan Yang and Wen-tau Yih and Xiaodong He and Jianfeng Gao and Li Deng},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6575},
  url={https://api.semanticscholar.org/CorpusID:2768038}
}
@inproceedings{wang2014knowledge,
  title={Knowledge graph embedding by translating on hyperplanes},
  author={Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={28},
  number={1},
  year={2014}
}
@inproceedings{ji2015knowledge,
  title={Knowledge graph embedding via dynamic mapping matrix},
  author={Ji, Guoliang and He, Shizhu and Xu, Liheng and Liu, Kang and Zhao, Jun},
  booktitle={Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: Long papers)},
  pages={687--696},
  year={2015}
}
@misc{englishidioms,
author={Dalzell, Tom},
year={2014},
  title = {English-language idioms},
  url = {https://en.wikipedia.org/wiki/English-language_idioms},
  urldate = {2023-05-27},
publisher={Wikipedia}
}
@inproceedings{clouatre-etal-2021-mlmlm,
    title = "{MLMLM}: Link Prediction with Mean Likelihood Masked Language Model",
    author = "Clouatre, Louis  and
      Trempe, Philippe  and
      Zouaq, Amal  and
      Chandar, Sarath",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.378",
    doi = "10.18653/v1/2021.findings-acl.378",
    pages = "4321--4331",
}
@article{pourpanah2022review,
  title={A review of generalized zero-shot learning methods},
  author={Pourpanah, Farhad and Abdar, Moloud and Luo, Yuxuan and Zhou, Xinlei and Wang, Ran and Lim, Chee Peng and Wang, Xi-Zhao and Wu, QM Jonathan},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2022},
  publisher={IEEE}
}
@article{blanco2015antiphrasis,
  title={Antiphrasis-based comparative constructional idioms in Spanish},
  author={Blanco, Carmen Mellado},
  journal={Journal of Social Sciences},
  volume={11},
  number={3},
  pages={111},
  year={2015},
  publisher={Citeseer}
}
@inproceedings{wang-etal-2022-simkgc,
    title = "{S}im{KGC}: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models",
    author = "Wang, Liang  and
      Zhao, Wei  and
      Wei, Zhuoyu  and
      Liu, Jingming",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.295",
    doi = "10.18653/v1/2022.acl-long.295",
    pages = "4281--4294",
    abstract = "Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19{\%} on WN18RR, +6.8{\%} on the Wikidata5M transductive setting, and +22{\%} on the Wikidata5M inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available at https://github.com/intfloat/SimKGC .",
}
@article{mcinnes2018umap,
  title={UMAP: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
  journal={The Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@inproceedings{zha2022inductive,
  title={Inductive relation prediction by BERT},
  author={Zha, Hanwen and Chen, Zhiyu and Yan, Xifeng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={5},
  pages={5923--5931},
  year={2022}
}
@article{oh2022open,
  title={Open-world knowledge graph completion for unseen entities and relations via attentive feature aggregation},
  author={Oh, Byungkook and Seo, Seungmin and Hwang, Jimin and Lee, Dongho and Lee, Kyong-Ho},
  journal={Information sciences},
  volume={586},
  pages={468--484},
  year={2022},
  publisher={Elsevier}
}
@inproceedings{keles2023computational,
  title={On the computational complexity of self-attention},
  author={Keles, Feyza Duman and Wijewardena, Pruthuvi Mahesakya and Hegde, Chinmay},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={597--619},
  year={2023},
  organization={PMLR}
}
@inproceedings{
    vashishth2020compositionbased,
    title={Composition-based Multi-Relational Graph Convolutional Networks},
    author={Shikhar Vashishth and Soumya Sanyal and Vikram Nitin and Partha Talukdar},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=BylA_C4tPr}
}
@inproceedings{Huang2023ATO,
  title={A Theory of Link Prediction via Relational Weisfeiler-Leman},
  author={Xingyue Huang and Miguel Romero and Ismail Ilkan Ceylan and Pablo Barcel{\'o}},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256616025}
}
@inproceedings{ingram,
	author={Jaejun Lee and Chanyoung Chung and Joyce Jiyoung Whang},
	title={{I}n{G}ram: Inductive Knowledge Graph Embedding via Relation Graphs},
	booktitle={Proceedings of the 40th International Conference on Machine Learning},
	year={2023},
	pages={18796--18809}
}
@inproceedings{jiang-etal-2023-text,
    title = "Text Augmented Open Knowledge Graph Completion via Pre-Trained Language Models",
    author = "Jiang, Pengcheng  and
      Agarwal, Shivam  and
      Jin, Bowen  and
      Wang, Xuan  and
      Sun, Jimeng  and
      Han, Jiawei",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.709",
    doi = "10.18653/v1/2023.findings-acl.709",
    pages = "11161--11180",
    abstract = "The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TagReal that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TagReal achieves state-of-the-art performance on two benchmark datasets. We find that TagReal has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.",
}
@inproceedings{Galkin2024TowardsFM,
  title={Towards Foundation Models for Knowledge Graph Reasoning},
  author={Mikhail Galkin and Xinyu Yuan and Hesham Mostafa and Jian Tang and Zhaocheng Zhu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:263831485}
}
@inproceedings{Zhu2021NeuralBN,
  title={Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction},
  author={Zhaocheng Zhu and Zuobai Zhang and Louis-Pascal Xhonneux and Jian Tang},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235422273}
}
@inproceedings{Galkin2021NodePieceCA,
  title={NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs},
  author={Mikhail Galkin and Jiapeng Wu and E. Denis and William L. Hamilton},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235606453}
}
@article{hogan2021knowledge,
  title={Knowledge graphs},
  author={Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d’Amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jos{\'e} Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and others},
  journal={ACM Computing Surveys (Csur)},
  volume={54},
  number={4},
  pages={1--37},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{ji2021survey,
  title={A survey on knowledge graphs: Representation, acquisition, and applications},
  author={Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={2},
  pages={494--514},
  year={2021},
  publisher={IEEE}
}
@inproceedings{xue-aletras-2023-pit,
    title = "Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention",
    author = "Xue, Huiyin  and
      Aletras, Nikolaos",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.695",
    doi = "10.18653/v1/2023.findings-emnlp.695",
    pages = "10355--10373",
    abstract = "Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.",
}
@inproceedings{Galkin2023TowardsFM,
  title={Towards Foundation Models for Knowledge Graph Reasoning},
  author={Mikhail Galkin and Xinyu Yuan and Hesham Mostafa and Jian Tang and Zhaocheng Zhu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  eprint={2310.04562},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{Huang2020CycleConsistentAA,
  title={Cycle-Consistent Adversarial Autoencoders for Unsupervised Text Style Transfer},
  author={Yufang Huang and Wentao Zhu and Deyi Xiong and Yiye Zhang and Changjian Hu and Feiyu Xu},
  booktitle={International Conference on Computational Linguistics},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:222125204}
}
@inproceedings{Bouamor2012IdentifyingBM,
  title={Identifying bilingual Multi-Word Expressions for Statistical Machine Translation},
  author={Dhouha Bouamor and Nasredine Semmar and Pierre Zweigenbaum},
  booktitle={International Conference on Language Resources and Evaluation},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:1260044}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{tam2022parameter,
    title = "Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers",
    author = "Tam, Weng  and
      Liu, Xiao  and
      Ji, Kaixuan  and
      Xue, Lilong  and
      Liu, Jiahua  and
      Li, Tao  and
      Dong, Yuxiao  and
      Tang, Jie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.874",
    doi = "10.18653/v1/2023.findings-emnlp.874",
    pages = "13117--13130",
    abstract = "Prompt tuning attempts to update few task-specific parameters in pre-trained models. It has achieved comparable performance to fine-tuning of the full parameter set on both language understanding and generation tasks. In this work, we study the problem of prompt tuning for neural text retrievers. We introduce parameter-efficient prompt tuning for text retrieval across in-domain, cross-domain, and cross-topic settings. Through an extensive analysis, we show that the strategy can mitigate the two issues{---}parameter-inefficiency and weak generalizability{---}faced by fine-tuning based retrieval methods. Notably, it can significantly improve the out-of-domain zero-shot generalization of the retrieval models. By updating only 0.1{\%} of the model parameters, the prompt tuning strategy can help retrieval models achieve better generalization performance than traditional methods in which all parameters are updated. Finally, to facilitate research on retrievers{'} cross-topic generalizability, we curate and release an academic retrieval dataset with 18K query-results pairs in 87 topics, making it the largest topic-specific one to date.",
}
@article{Zhong2020BituningOP,
  title={Bi-tuning of Pre-trained Representations},
  author={Jincheng Zhong and Ximei Wang and Zhi Kou and Jianmin Wang and Mingsheng Long},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.06182},
  url={https://api.semanticscholar.org/CorpusID:226307145}
}
@inproceedings{gesese2022raild,
author = {Gesese, Genet Asefa and Sack, Harald and Alam, Mehwish},
title = {RAILD: Towards Leveraging Relation Features for Inductive Link Prediction In Knowledge Graphs},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579066},
doi = {10.1145/3579051.3579066},
abstract = {Due to the open world assumption, Knowledge Graphs (KGs) are never complete. In order to address this issue, various Link Prediction (LP) methods are proposed so far. Some of these methods are inductive LP models which are capable of learning representations for entities not seen during training. However, to the best of our knowledge, none of the existing inductive LP models focus on learning representations for unseen relations. In this work, a novel Relation Aware Inductive Link preDiction (RAILD) is proposed for KG completion which learns representations for both unseen entities and unseen relations. In addition to leveraging textual literals associated with both entities and relations by employing language models, RAILD also introduces a novel graph-based approach to generate features for relations. Experiments are conducted with different existing and newly created challenging benchmark datasets and the results indicate that RAILD leads to performance improvement over the state-of-the-art models. Moreover, since there are no existing inductive LP models which learn representations for unseen relations, we have created our own baselines and the results obtained with RAILD also outperform these baselines.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {82–90},
numpages = {9},
series = {IJCKG '22}
}
@inproceedings{markowitz2022statik,
  title={StATIK: Structure and text for inductive knowledge graph completion},
  author={Markowitz, Elan and Balasubramanian, Keshav and Mirtaheri, Mehrnoosh and Annavaram, Murali and Galstyan, Aram and Ver Steeg, Greg},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2022},
  pages={604--615},
  year={2022}
}
@article{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{american2016ethical,
  title={Ethical principles of psychologists and code of conduct},
  author={American Psychological Association and others},
  year={2016},
  publisher={APA}
}
@inproceedings{Reimers2019SentenceBERTSE,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Nils Reimers and Iryna Gurevych},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:201646309}
}
@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}
@inproceedings{inductiveEntityRepresentation,
author = {Daza, Daniel and Cochez, Michael and Groth, Paul},
title = {Inductive Entity Representations from Text via Link Prediction},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450141},
doi = {10.1145/3442381.3450141},
abstract = {Knowledge Graphs (KG) are of vital importance for multiple applications on the web, including information retrieval, recommender systems, and metadata annotation. Regardless of whether they are built manually by domain experts or with automatic pipelines, KGs are often incomplete. To address this problem, there is a large amount of work that proposes using machine learning to complete these graphs by predicting new links. Recent work has begun to explore the use of textual descriptions available in knowledge graphs to learn vector representations of entities in order to preform link prediction. However, the extent to which these representations learned for link prediction generalize to other tasks is unclear. This is important given the cost of learning such representations. Ideally, we would prefer representations that do not need to be trained again when transferring to a different task, while retaining reasonable performance. Therefore, in this work, we propose a holistic evaluation protocol for entity representations learned via a link prediction objective. We consider the inductive link prediction and entity classification tasks, which involve entities not seen during training. We also consider an information retrieval task for entity-oriented search. We evaluate an architecture based on a pretrained language model, that exhibits strong generalization to entities not observed during training, and outperforms related state-of-the-art methods (22\% MRR improvement in link prediction on average). We further provide evidence that the learned representations transfer well to other tasks without fine-tuning. In the entity classification task we obtain an average improvement of 16\% in accuracy compared with baselines that also employ pre-trained models. In the information retrieval task, we obtain significant improvements of up to 8.8\% in NDCG@10 for natural language queries. We thus show that the learned representations are not limited KG-specific tasks, and have greater generalization properties than evaluated in previous work.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {798–808},
numpages = {11},
keywords = {knowledge graphs, information retrieval, link prediction, entity classification, entity representations},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}
@inproceedings{hamaguchi2017knowledge,
author = {Hamaguchi, Takuo and Oiwa, Hidekazu and Shimbo, Masashi and Matsumoto, Yuji},
title = {Knowledge transfer for out-of-knowledge-base entities: a graph neural network approach},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Knowledge base completion (KBC) aims to predict missing information in a knowledge base. In this paper, we address the out-of-knowledge-base (OOKB) entity problem in KBC: how to answer queries concerning test entities not observed at training time. Existing embedding-based KBC models assume that all test entities are available at training time, making it unclear how to obtain embeddings for new entities without costly retraining. To solve the OOKB entity problem without retraining, we use graph neural networks (Graph-NNs) to compute the embeddings of OOKB entities, exploiting the limited auxiliary knowledge provided at test time. The experimental results show the effectiveness of our proposed model in the OOKB setting. Additionally, in the standard KBC setting in which OOKB entities are not involved, our model achieves state-of-the-art performance on the WordNet dataset.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {1802–1808},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}
@article{humeau2019poly,
  title={Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring},
  author={Humeau, Samuel and Shuster, Kurt and Lachaux, Marie-Anne and Weston, Jason},
  journal={arXiv preprint arXiv:1905.01969},
  year={2019}
}
@article{galkin2021nodepiece,
  title={Nodepiece: Compositional and parameter-efficient representations of large knowledge graphs},
  author={Galkin, Mikhail and Wu, Jiapeng and Denis, Etienne and Hamilton, William L},
  journal={arXiv preprint arXiv:2106.12144},
  year={2021}
}
@article{tian2020makes,
  title={What makes for good views for contrastive learning?},
  author={Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6827--6839},
  year={2020}
}
@inproceedings{xie2016representation,
  title={Representation learning of knowledge graphs with entity descriptions},
  author={Xie, Ruobing and Liu, Zhiyuan and Jia, Jia and Luan, Huanbo and Sun, Maosong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}
@inproceedings{balazevic-etal-2019-tucker,
    title = "{T}uck{ER}: Tensor Factorization for Knowledge Graph Completion",
    author = "Balazevic, Ivana  and
      Allen, Carl  and
      Hospedales, Timothy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1522",
    doi = "10.18653/v1/D19-1522",
    pages = "5185--5194",
}
@article{yao2019kg,
  title={KG-BERT: BERT for knowledge graph completion},
  author={Yao, Liang and Mao, Chengsheng and Luo, Yuan},
  journal={arXiv preprint arXiv:1909.03193},
  year={2019}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@inproceedings{akrami2020realistic,
  title={Realistic re-evaluation of knowledge graph completion methods: An experimental study},
  author={Akrami, Farahnaz and Saeef, Mohammed Samiul and Zhang, Qingheng and Hu, Wei and Li, Chengkai},
  booktitle={Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
  pages={1995--2010},
  year={2020}
}
@inproceedings{jiang2022promptbert,
    title = "{P}rompt{BERT}: Improving {BERT} Sentence Embeddings with Prompts",
    author = "Jiang, Ting  and
      Jiao, Jian  and
      Huang, Shaohan  and
      Zhang, Zihan  and
      Wang, Deqing  and
      Zhuang, Fuzhen  and
      Wei, Furu  and
      Huang, Haizhen  and
      Deng, Denvy  and
      Zhang, Qi",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.603",
    doi = "10.18653/v1/2022.emnlp-main.603",
    pages = "8826--8837",
    abstract = "We propose PromptBERT, a novel contrastive learning method for learning better sentence representation. We firstly analysis the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers. Then we propose the first prompt-based sentence embeddings method and discuss two prompt representing methods and three prompt searching methods to make BERT achieve better sentence embeddings .Moreover, we propose a novel unsupervised training objective by the technology of template denoising, which substantially shortens the performance gap between the supervised and unsupervised settings. Extensive experiments show the effectiveness of our method. Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.",
}
@article{marcus-etal-1993-building,
    title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
    author = "Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J93-2004",
    pages = "313--330",
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}
@article{li2020dividemix,
  title={Dividemix: Learning with noisy labels as semi-supervised learning},
  author={Li, Junnan and Socher, Richard and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2002.07394},
  year={2020}
}
@article{wu2019scalable,
  title={Scalable zero-shot entity linking with dense entity retrieval},
  author={Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1911.03814},
  year={2019}
}
@inproceedings{agarwal-etal-2022-entity,
    title = "Entity Linking via Explicit Mention-Mention Coreference Modeling",
    author = "Agarwal, Dhruv  and
      Angell, Rico  and
      Monath, Nicholas  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.343",
    doi = "10.18653/v1/2022.naacl-main.343",
    pages = "4644--4658",
    abstract = "Learning representations of entity mentions is a core component of modern entity linking systems for both candidate generation and making linking predictions. In this paper, we present and empirically analyze a novel training approach for learning mention and entity representations that is based on building minimum spanning arborescences (i.e., directed spanning trees) over mentions and entities across documents to explicitly model mention coreference relationships. We demonstrate the efficacy of our approach by showing significant improvements in both candidate generation recall and linking accuracy on the Zero-Shot Entity Linking dataset and MedMentions, the largest publicly available biomedical dataset. In addition, we show that our improvements in candidate generation yield higher quality re-ranking models downstream, setting a new SOTA result in linking accuracy on MedMentions. Finally, we demonstrate that our improved mention representations are also effective for the discovery of new entities via cross-document coreference.",
}
@inproceedings{gao2021simcse,
  title={SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:233296292}
}
@inproceedings{zbontar2021barlow,
  title={Barlow twins: Self-supervised learning via redundancy reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  booktitle={International Conference on Machine Learning},
  pages={12310--12320},
  year={2021},
  organization={PMLR}
}
@article{wu2021representing,
  title={Representing long-range context for graph neural networks with global attention},
  author={Wu, Zhanghao and Jain, Paras and Wright, Matthew and Mirhoseini, Azalia and Gonzalez, Joseph E and Stoica, Ion},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13266--13279},
  year={2021}
}
@article{ying2021transformers,
  title={Do transformers really perform badly for graph representation?},
  author={Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28877--28888},
  year={2021}
}
@inproceedings{yang2023dual,
  title={A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking},
  author={Yang, Yuting and Lei, Wenqiang and Huang, Pei and Cao, Juan and Li, Jintao and Chua, Tat-Seng},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={1468--1477},
  year={2023}
}
@inproceedings{zhou2022dual,
  title={Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning},
  author={Zhou, Jie and Tian, Le and Yu, Houjin and Xiao, Zhou and Su, Hui},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={79--84},
  year={2022}
}
@inproceedings{liu2021p,
    title = "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    author = "Liu, Xiao  and
      Ji, Kaixuan  and
      Fu, Yicheng  and
      Tam, Weng  and
      Du, Zhengxiao  and
      Yang, Zhilin  and
      Tang, Jie",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.8",
    doi = "10.18653/v1/2022.acl-short.8",
    pages = "61--68",
    abstract = "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1{\%}-3{\%} tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@inproceedings{pezeshkpour2019investigating,
  title={Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications},
  author={Pouya Pezeshkpour and Yifan Tian and Sameer Singh},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:86813509}
}
@inproceedings{tenney2019you,
  title={What do you learn from context? Probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R. and Das, Dipanjan and others},
  booktitle={Proceedings of the 7th International Conference on Learning Representations (ICLR)},
  year={2019},
  url={https://arxiv.org/abs/1905.06316}
}
@article{reiter1980logic,
  title={A logic for default reasoning},
  author={Reiter, Raymond},
  journal={Artificial intelligence},
  volume={13},
  number={1-2},
  pages={81--132},
  year={1980},
  publisher={Elsevier}
}

@inproceedings{chen2022probing,
  title={Probing Linguistic Information For Logical Inference In Pre-trained Language Models},
  author={Chen, Zeming and Gao, Qiyue},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={10509--10517},
  year={2022}
}
@inproceedings{bollacker2008freebase,
  title={Freebase: a collaboratively created graph database for structuring human knowledge},
  author={Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  booktitle={Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
  pages={1247--1250},
  year={2008}
}
@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}
@inproceedings{ma2022open,
  title={Open Domain Question Answering with A Unified Knowledge Interface},
  author={Ma, Kaixin and Cheng, Hao and Liu, Xiaodong and Nyberg, Eric and Gao, Jianfeng},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1605--1620},
  year={2022}
}
@book{de1975clelia,
  title={Clelia, an Excellent New Romance the Whole Work in Five Parts, Dedicated to Mademoiselle de Longueville},
  author={de Scud{\'e}ry, Madeleine},
  year={1975},
  publisher={University of Michigan}
}
@article{rossi2021knowledge,
  title={Knowledge graph embedding for link prediction: A comparative analysis},
  author={Rossi, Andrea and Barbosa, Denilson and Firmani, Donatella and Matinata, Antonio and Merialdo, Paolo},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={15},
  number={2},
  pages={1--49},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{radev2002evaluating,
  title={Evaluating Web-based Question Answering Systems.},
  author={Radev, Dragomir R and Qi, Hong and Wu, Harris and Fan, Weiguo},
  booktitle={LREC},
  year={2002},
  organization={Citeseer}
}
@inproceedings{Saxena2022SequencetoSequenceKG,
  title={Sequence-to-Sequence Knowledge Graph Completion and Question Answering},
  author={Apoorv Saxena and Adrian Kochsiek and Rainer Gemulla},
  booktitle={ACL},
  year={2022}
}
@inproceedings{hu-etal-2022-knowledgeable,
    title = "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification",
    author = "Hu, Shengding  and
      Ding, Ning  and
      Wang, Huadong  and
      Liu, Zhiyuan  and
      Wang, Jingang  and
      Li, Juanzi  and
      Wu, Wei  and
      Sun, Maosong",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.158",
    doi = "10.18653/v1/2022.acl-long.158",
    pages = "2225--2240",
    abstract = "Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompttuning (KPT), to improve and stabilize prompttuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.",
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}
@inproceedings{lu2020twinbert,
  title={Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval},
  author={Lu, Wenhao and Jiao, Jian and Zhang, Ruofei},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={2645--2652},
  year={2020}
}
@inproceedings{dettmers2018convolutional,
  title={Convolutional 2d knowledge graph embeddings},
  author={Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}
@inproceedings{Santorini1990PartofspeechTG,
  title={Part-of-speech tagging guidelines for the penn treebank project},
  author={Beatrice Santorini},
  year={1990}
}
@article{chen2020review,
  title={A review: Knowledge reasoning over knowledge graph},
  author={Chen, Xiaojun and Jia, Shengbin and Xiang, Yang},
  journal={Expert Systems with Applications},
  volume={141},
  pages={112948},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{hwang2021comet,
  title={(Comet-) atomic 2020: on symbolic and neural commonsense knowledge graphs},
  author={Hwang, Jena D and Bhagavatula, Chandra and Le Bras, Ronan and Da, Jeff and Sakaguchi, Keisuke and Bosselut, Antoine and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={7},
  pages={6384--6392},
  year={2021}
}
@inproceedings{petroni-etal-2019-language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at \url{https://github.com/facebookresearch/LAMA}.",
}

@article{weber1996attributes,
  title={Are attributes entities? A study of database designers' memory structures},
  author={Weber, Ron},
  journal={Information Systems Research},
  volume={7},
  number={2},
  pages={137--162},
  year={1996},
  publisher={INFORMS}
}
@inproceedings{xiong-etal-2017-deeppath,
    title = "{D}eep{P}ath: A Reinforcement Learning Method for Knowledge Graph Reasoning",
    author = "Xiong, Wenhan  and
      Hoang, Thien  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1060",
    doi = "10.18653/v1/D17-1060",
    pages = "564--573",
    abstract = "We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.",
}
@article{lee2022selective,
  title={Selective compression learning of latent representations for variable-rate image compression},
  author={Lee, Jooyoung and Jeong, Seyoon and Kim, Munchurl},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13146--13157},
  year={2022}
}
